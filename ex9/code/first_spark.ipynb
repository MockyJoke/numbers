{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n",
      "| id|    x|    y|     z|\n",
      "+---+-----+-----+------+\n",
      "|  2|84.46| 6.69|254.14|\n",
      "|  6|40.48|29.49|122.14|\n",
      "| 10|58.72| 0.77|176.91|\n",
      "| 14|88.65|11.28| 266.8|\n",
      "| 18|74.21|46.04| 223.5|\n",
      "| 22| 91.8|31.11|276.46|\n",
      "| 26|52.55| 5.15|159.05|\n",
      "| 30|74.76|33.39|224.85|\n",
      "| 34|41.58| 3.37|125.34|\n",
      "| 38|34.43|26.08|104.57|\n",
      "| 42|33.58|44.48|101.43|\n",
      "| 46|56.65|16.08|171.43|\n",
      "| 50|53.59|14.41|161.49|\n",
      "| 54|19.82| 23.0| 60.24|\n",
      "| 58|55.53| 8.66|167.34|\n",
      "| 62|24.45| 0.01| 74.59|\n",
      "| 66|16.43| 37.5| 50.29|\n",
      "| 70|39.49| 27.3|119.71|\n",
      "| 74|74.77|42.21|225.21|\n",
      "| 78|46.01|15.77|138.61|\n",
      "+---+-----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, functions, types\n",
    "\n",
    "spark = SparkSession.builder.appName('first Spark app').getOrCreate()\n",
    "\n",
    "assert sys.version_info >= (3, 4) # make sure we have Python 3.4+\n",
    "assert spark.version >= '2.1' # make sure we have Spark 2.1+\n",
    "\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('id', types.IntegerType(), False),\n",
    "    types.StructField('x', types.FloatType(), False),\n",
    "    types.StructField('y', types.FloatType(), False),\n",
    "    types.StructField('z', types.FloatType(), False),\n",
    "])\n",
    "\n",
    "\n",
    "def main():\n",
    "    in_directory = sys.argv[1]\n",
    "    out_directory = sys.argv[2]\n",
    "    in_directory = \"xyz-1\"\n",
    "    out_directory = \"output\"\n",
    "    # Read the data from the JSON files\n",
    "    xyz = spark.read.json(in_directory, schema=schema)\n",
    "#     xyz.show(); return\n",
    "    xyz.show()\n",
    "    # Create a DF with what we need: x, (soon y,) and id%10 which we'll aggregate by.\n",
    "    with_bins = xyz.select(\n",
    "        xyz['x'],\n",
    "        # TODO: also the y values\n",
    "        xyz['y'],\n",
    "        (xyz['id'] % 10).alias('bin'),\n",
    "        \n",
    "    )\n",
    "    #with_bins.show(); return\n",
    "\n",
    "    # Aggregate by the bin number.\n",
    "    grouped = with_bins.groupBy(with_bins['bin'])\n",
    "    groups = grouped.agg(\n",
    "        functions.sum(with_bins['x']),\n",
    "        # TODO: output the average y value. Hint: avg\n",
    "        functions.avg(xyz['y']),\n",
    "\n",
    "        functions.count('*'))\n",
    "    # We know groups has <=10 rows, so it can safely be moved into two partitions.\n",
    "    groups = groups.sort(groups['bin']).coalesce(2)\n",
    "\n",
    "    groups.write.csv(out_directory, compression=None, mode='overwrite')\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
