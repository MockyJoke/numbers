{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+--------+------------------+------------+\n",
      "|  n|x_sum|      y_sum|x_sq_sum|          y_sq_sum|      xy_sum|\n",
      "+---+-----+-----------+--------+------------------+------------+\n",
      "|232| 1972|3.6133736E7| 32560.0|2.5731257461526E13|6.62179733E8|\n",
      "+---+-----+-----------+--------+------------------+------------+\n",
      "\n",
      "r = 0.630006\n",
      "r^2 = 0.396907\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, functions, types, Row\n",
    "import re\n",
    "import math\n",
    "\n",
    "spark = SparkSession.builder.appName('correlate logs').getOrCreate()\n",
    "\n",
    "assert sys.version_info >= (3, 4) # make sure we have Python 3.4+\n",
    "assert spark.version >= '2.1' # make sure we have Spark 2.1+\n",
    "\n",
    "line_re = re.compile(\"^(\\\\S+) - - \\\\[\\\\S+ [+-]\\\\d+\\\\] \\\"[A-Z]+ \\\\S+ HTTP/\\\\d\\\\.\\\\d\\\" \\\\d+ (\\\\d+)$\")\n",
    "\n",
    "\n",
    "def line_to_row(line):\n",
    "    \"\"\"\n",
    "    Take a logfile line and return a Row object with hostname and bytes transferred. Return None if regex doesn't match.\n",
    "    \"\"\"\n",
    "    m = line_re.match(line)\n",
    "    if m:\n",
    "        # TODO\n",
    "        return Row(hostname=m.group(1), transferred = m.group(2))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def not_none(row):\n",
    "    \"\"\"\n",
    "    Is this None? Hint: .filter() with it.\n",
    "    \"\"\"\n",
    "    return row is not None\n",
    "\n",
    "\n",
    "def create_row_rdd(in_directory):\n",
    "    log_lines = spark.sparkContext.textFile(in_directory)\n",
    "    # TODO: return an RDD of Row() objects\n",
    "    return log_lines.map(line_to_row).filter(not_none)\n",
    "\n",
    "\n",
    "def main():\n",
    "    in_directory = sys.argv[1]\n",
    "    \n",
    "    in_directory = \"nasa-logs-1\"\n",
    "    \n",
    "    logs = spark.createDataFrame(create_row_rdd(in_directory)).cache()\n",
    "    \n",
    "    log_by_hostname = logs.groupby(\"hostname\").agg(functions.count(\"hostname\"),functions.sum(\"transferred\"))\n",
    "\n",
    "    df_stat = log_by_hostname.groupby().agg(\n",
    "        functions.count(\"hostname\").alias(\"n\"),\n",
    "        functions.sum(\"count(hostname)\").alias(\"x_sum\"),\n",
    "        functions.sum(\"sum(transferred)\").alias(\"y_sum\"),\n",
    "        functions.sum(log_by_hostname[\"count(hostname)\"]**2).alias(\"x_sq_sum\"),\n",
    "        functions.sum(log_by_hostname[\"sum(transferred)\"]**2).alias(\"y_sq_sum\"),\n",
    "        functions.sum(log_by_hostname[\"count(hostname)\"] * log_by_hostname[\"sum(transferred)\"]).alias(\"xy_sum\")\n",
    "    )\n",
    "    df_stat.show()\n",
    "    stat = df_stat.first()\n",
    "    r = (stat.n * stat.xy_sum - stat.x_sum * stat.y_sum) / (math.sqrt(stat.n * stat.x_sq_sum - stat.x_sum**2) * math.sqrt(stat.n * stat.y_sq_sum - stat.y_sum**2 ))\n",
    "#     r = 0 # TODO: it isn't zero.\n",
    "    print(\"r = %g\\nr^2 = %g\" % (r, r**2))\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
